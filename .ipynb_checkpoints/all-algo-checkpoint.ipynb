{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Canteen.mp4\"\n",
    "frame_width, frame_height = 1280,720\n",
    "# modelPath = \"./yolo-head-detection.pt\"\n",
    "modelPath = \"./yolov8s.pt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOTSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-04 05:23:35.413\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\u001b[0m\n",
      "\u001b[32m2023-11-04 05:23:35.414\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m211\u001b[0m - \u001b[33m\u001b[1mThe following layers are discarded due to unmatched keys or layer size: ('classifier.weight', 'classifier.bias')\u001b[0m\n",
      "WARNING ⚠️ 'source' is missing. Using 'source=/home/loki/Development/major-project/RealTime-Webcam/venv/lib/python3.11/site-packages/ultralytics/assets'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "botsort_Canteen.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from boxmot import BoTSORT\n",
    "from pathlib import Path\n",
    "frame_width, frame_height = 1280,720\n",
    "model = YOLO(modelPath)\n",
    "\n",
    "def calculate_color_based_on_id(id):\n",
    "  colors = [[255, 0, 0], # Red\n",
    "          [0, 255, 0], # Green|\n",
    "          [0, 0, 255], # Blue\n",
    "          [255, 255, 0], # Yellow\n",
    "          [255, 0, 255], # Magenta\n",
    "          [0, 255, 255], # Cyan\n",
    "          [128, 128, 128], # Gray\n",
    "          [255, 255, 255], # White\n",
    "          [0, 0, 0], # Black\n",
    "          [192, 192, 192]]\n",
    "  return colors[id % len(colors) ]\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(f\"./input_videos/{filename}\")\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_height)\n",
    "ret, frame = cap.read() \n",
    "tracker = BoTSORT(model_weights=Path('osnet_x0_25_msmt17.pt'),device=\"cuda:0\",fp16=False,track_buffer=60*15,frame_rate=60)\n",
    "thickness = 2\n",
    "color = (0,0,255)\n",
    "# cap_out = cv2.VideoWriter(f\"output_videos/botsort_{filename}\", cv2.VideoWriter_fourcc(*'MP4V'), cap.get(cv2.CAP_PROP_FPS),\n",
    "#                           (frame.shape[1], frame.shape[0]))\n",
    "count = 0\n",
    "id_set = set()\n",
    "while(cap.isOpened()):\n",
    "    count +=1 \n",
    "    detections = model.predict(frame,verbose=False)[0]\n",
    "    results = []\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if np.array(detections.boxes.data.tolist()).ndim < 2:\n",
    "        results = [[0, 0, 0, 0, 0.0922948837280273, 0]] \n",
    "    try:\n",
    "        ts = tracker.update(np.array(detections.boxes.data.tolist()), frame)\n",
    "        xyxys = ts[:,0:4].astype('int') # float64 to int\n",
    "        ids = ts[:, 4].astype('int') # float64 to int \n",
    "        confs = ts[:, 5]\n",
    "        clss = ts[:, 6]\n",
    "        if ts.shape[0] != 0:\n",
    "            for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n",
    "\n",
    "                if cls != 0:\n",
    "                    continue\n",
    "                id_set.add(id)\n",
    "                cv2.rectangle(\n",
    "                    frame,\n",
    "                    (xyxy[0], xyxy[1]),\n",
    "                    (xyxy[2], xyxy[3]),\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f'id: {id}',\n",
    "                    (xyxy[0], xyxy[1]-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "            cv2.putText(\n",
    "                    frame,\n",
    "                    f'Count: {len(id_set)}',\n",
    "                    (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0,0,0),\n",
    "                    thickness\n",
    "                )\n",
    "            cv2.putText(\n",
    "                    frame,\n",
    "                    f'Frame: {count}',\n",
    "                    (10, 60),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0,0,0),\n",
    "                    thickness\n",
    "                )\n",
    "            \n",
    "            cv2.imshow('Frame', frame)\n",
    "\n",
    "            # cap_out.write(frame)\n",
    "\n",
    "    except Exception as e:\n",
    "         print(\"Error:\",e)\n",
    "         pass\n",
    "    ret, frame = cap.read()         \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "            break\n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows() \n",
    "print(f\"botsort_{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-04 04:25:41.993\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\u001b[0m\n",
      "\u001b[32m2023-11-04 04:25:41.994\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m211\u001b[0m - \u001b[33m\u001b[1mThe following layers are discarded due to unmatched keys or layer size: ('classifier.weight', 'classifier.bias')\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "15\n",
      "15\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "14\n",
      "14\n",
      "14\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "13\n",
      "13\n",
      "15\n",
      "15\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "10\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "14\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'source' is missing. Using 'source=/home/loki/Development/major-project/RealTime-Webcam/venv/lib/python3.11/site-packages/ultralytics/assets'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "botsort_Canteen.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from boxmot import BoTSORT\n",
    "from pathlib import Path\n",
    "frame_width, frame_height = 1280,720\n",
    "model = YOLO(\"yolo-head-detection.pt\")\n",
    "\n",
    "def calculate_color_based_on_id(id):\n",
    "  colors = [[255, 0, 0], # Red\n",
    "          [0, 255, 0], # Green|\n",
    "          [0, 0, 255], # Blue\n",
    "          [255, 255, 0], # Yellow\n",
    "          [255, 0, 255], # Magenta\n",
    "          [0, 255, 255], # Cyan\n",
    "          [128, 128, 128], # Gray\n",
    "          [255, 255, 255], # White\n",
    "          [0, 0, 0], # Black\n",
    "          [192, 192, 192]]\n",
    "  return colors[id % len(colors) ]\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(f\"./input_videos/{filename}\")\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_height)\n",
    "ret, frame = cap.read() \n",
    "tracker = BoTSORT(model_weights=Path('osnet_x0_25_msmt17.pt'),device=\"cuda:0\",fp16=False)\n",
    "thickness = 2\n",
    "color = (0,0,255)\n",
    "# cap_out = cv2.VideoWriter(f\"output_videos/botsort_{filename}\", cv2.VideoWriter_fourcc(*'MP4V'), cap.get(cv2.CAP_PROP_FPS),\n",
    "#                           (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    id_set = set()\n",
    "    detections = model.predict(frame,verbose=False)[0]\n",
    "    results = []\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if np.array(detections.boxes.data.tolist()).ndim < 2:\n",
    "        results = [[0, 0, 0, 0, 0.0922948837280273, 0]] \n",
    "    try:\n",
    "        ts = tracker.update(np.array(detections.boxes.data.tolist()), frame)\n",
    "        xyxys = ts[:,0:4].astype('int') # float64 to int\n",
    "        ids = ts[:, 4].astype('int') # float64 to int \n",
    "        confs = ts[:, 5]\n",
    "        clss = ts[:, 6]\n",
    "        if ts.shape[0] != 0:\n",
    "            for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n",
    "\n",
    "                if cls != 0:\n",
    "                    continue\n",
    "                id_set.add(id)\n",
    "                cv2.rectangle(\n",
    "                    frame,\n",
    "                    (xyxy[0], xyxy[1]),\n",
    "                    (xyxy[2], xyxy[3]),\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f'id: {id}',\n",
    "                    (xyxy[0], xyxy[1]-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "            print(len(id_set))\n",
    "            cv2.putText(\n",
    "                    frame,\n",
    "                    f'Count: {len(id_set)}',\n",
    "                    (10, 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0,0,0),\n",
    "                    thickness\n",
    "                )\n",
    "            cv2.imshow('Frame', frame)\n",
    "            # cap_out.write(frame)\n",
    "\n",
    "    except Exception as e:\n",
    "         print(\"Error:\",e)\n",
    "         pass\n",
    "    ret, frame = cap.read()         \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "            break\n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows() \n",
    "print(f\"botsort_{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StrongSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-04 04:52:48.231\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\u001b[0m\n",
      "\u001b[32m2023-11-04 04:52:48.232\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m211\u001b[0m - \u001b[33m\u001b[1mThe following layers are discarded due to unmatched keys or layer size: ('classifier.weight', 'classifier.bias')\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: too many indices for array: array is 1-dimensional, but 2 were indexed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'source' is missing. Using 'source=/home/loki/Development/major-project/RealTime-Webcam/venv/lib/python3.11/site-packages/ultralytics/assets'.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from boxmot import BoTSORT,StrongSORT\n",
    "from pathlib import Path\n",
    "model = YOLO(modelPath)\n",
    "\n",
    "def calculate_color_based_on_id(id):\n",
    "  colors = [[255, 0, 0], # Red\n",
    "          [0, 255, 0], # Green\n",
    "          [0, 0, 255], # Blue\n",
    "          [255, 255, 0], # Yellow\n",
    "          [255, 0, 255], # Magenta\n",
    "          [0, 255, 255], # Cyan\n",
    "          [128, 128, 128], # Gray\n",
    "          [255, 255, 255], # White\n",
    "          [0, 0, 0], # Black\n",
    "          [192, 192, 192]]\n",
    "  return colors[id % len(colors) ]\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(f\"./input_videos/{filename}\")\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_height)\n",
    "ret, frame = cap.read() \n",
    "tracker = StrongSORT(model_weights=Path('osnet_x0_25_msmt17.pt'),device=\"cuda:0\",fp16=False)\n",
    "thickness = 2\n",
    "color = (0,0,255)\n",
    "while(cap.isOpened()):\n",
    "    id_set = set()\n",
    "    detections = model.predict(frame,verbose=False)[0]\n",
    "    results = []\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if np.array(detections.boxes.data.tolist()).ndim < 2:\n",
    "        results = [[0, 0, 0, 0, 0.0922948837280273, 0]] \n",
    "    try:\n",
    "        ts = tracker.update(np.array(detections.boxes.data.tolist()), frame)\n",
    "        xyxys = ts[:,0:4].astype('int') # float64 to int\n",
    "        ids = ts[:, 4].astype('int') # float64 to int \n",
    "        confs = ts[:, 5]\n",
    "        clss = ts[:, 6]\n",
    "        if ts.shape[0] != 0:\n",
    "            for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n",
    "\n",
    "                if cls != 0:\n",
    "                    continue\n",
    "                id_set.add(id)\n",
    "                cv2.rectangle(\n",
    "                    frame,\n",
    "                    (xyxy[0], xyxy[1]),\n",
    "                    (xyxy[2], xyxy[3]),\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f'id: {id}',\n",
    "                    (xyxy[0], xyxy[1]-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "            cv2.putText(\n",
    "                    frame,\n",
    "                    f'Count: {len(id_set)}',\n",
    "                    (10, 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0,0,0),\n",
    "                    thickness\n",
    "                )\n",
    "            cv2.imshow('Frame', frame)\n",
    "    except Exception as e:\n",
    "         print(\"Error:\",e)\n",
    "         pass\n",
    "    ret, frame = cap.read()         \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "            break\n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ByteTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "QObject::moveToThread: Current thread (0x55b351395560) is not the object's thread (0x55b350f145c0).\n",
      "Cannot move to target thread (0x55b351395560)\n",
      "\n",
      "qt.qpa.plugin: Could not load the Qt platform plugin \"wayland\" in \"/home/loki/Development/major-project/RealTime-Webcam/venv/lib/python3.11/site-packages/cv2/qt/plugins\" even though it was found.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import argparse\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(f\"./input_videos/{filename}\")\n",
    "# cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_height)\n",
    "\n",
    "\n",
    "# model = YOLO('yolo-head-detection.pt')\n",
    "model = YOLO('yolov8l.pt')\n",
    "\n",
    "CLASS_NAMES_DICT = model.model.names\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "# mask_annotator = sv.MaskAnnotator()\n",
    "tracker = sv.ByteTrack(frame_rate=30,track_buffer=120)\n",
    "print(model.model.names)\n",
    "\n",
    "ret,frame = cap.read()\n",
    "cap_out = cv2.VideoWriter(f\"out.mp4\", cv2.VideoWriter_fourcc(*'MP4V'), cap.get(cv2.CAP_PROP_FPS),\n",
    "                      (frame.shape[1], frame.shape[0]))\n",
    "while ret:\n",
    "    ids = set()\n",
    "    result = model(frame, verbose=False)[0]\n",
    "    detections = sv.Detections.from_ultralytics(result)  \n",
    "    detections = detections[(detections.class_id==0)]\n",
    "    detections = tracker.update_with_detections(detections=detections)\n",
    "    labels  = []\n",
    "    for _, _, confidence, class_id, tracker_id in detections:\n",
    "        labels.append(f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\")\n",
    "        ids.add(tracker_id)\n",
    "    \n",
    "    annotated_image = box_annotator.annotate(frame, detections=detections,labels=labels)\n",
    "    # annotated_image = mask_annotator.annotate(scene=frame,detections=detections,opacity=0.5)\n",
    "    cv2.putText(\n",
    "                    annotated_image,\n",
    "                    f\"Count {len(ids)}\",\n",
    "                    (10, 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0,0,0),\n",
    "                    2\n",
    "                )\n",
    "    cv2.imshow(\"YoloV8\",annotated_image)\n",
    "    cap_out.write(annotated_image)\n",
    "    # ESC -> 27\n",
    "    if cv2.waitKey(30) == 27:\n",
    "        break\n",
    "    ret,frame = cap.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep SORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-04 04:41:13.020\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\u001b[0m\n",
      "\u001b[32m2023-11-04 04:41:13.021\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m211\u001b[0m - \u001b[33m\u001b[1mThe following layers are discarded due to unmatched keys or layer size: ('classifier.weight', 'classifier.bias')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from boxmot import BoTSORT,StrongSORT,DeepOCSORT\n",
    "from pathlib import Path\n",
    "model = YOLO(\"./yolo-head-detection.pt\")\n",
    "\n",
    "def calculate_color_based_on_id(id):\n",
    "  colors = [[255, 0, 0], # Red\n",
    "          [0, 255, 0], # Green\n",
    "          [0, 0, 255], # Blue\n",
    "          [255, 255, 0], # Yellow\n",
    "          [255, 0, 255], # Magenta\n",
    "          [0, 255, 255], # Cyan\n",
    "          [128, 128, 128], # Gray\n",
    "          [255, 255, 255], # White\n",
    "          [0, 0, 0], # Black\n",
    "          [192, 192, 192]]\n",
    "  return colors[id % len(colors) ]\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(f\"./input_videos/{filename}\")\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_height)\n",
    "ret, frame = cap.read() \n",
    "tracker = DeepOCSORT(model_weights=Path('osnet_x0_25_msmt17.pt'),device=\"cuda:0\",fp16=False)\n",
    "thickness = 2\n",
    "color = (0,0,255)\n",
    "while(cap.isOpened()):\n",
    "    id_set = set()\n",
    "    detections = model.predict(frame,verbose=False)[0]\n",
    "    results = []\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if np.array(detections.boxes.data.tolist()).ndim < 2:\n",
    "        results = [[0, 0, 0, 0, 0.0922948837280273, 0]] \n",
    "    try:\n",
    "        ts = tracker.update(np.array(detections.boxes.data.tolist()), frame)\n",
    "        xyxys = ts[:,0:4].astype('int') # float64 to int\n",
    "        ids = ts[:, 4].astype('int') # float64 to int \n",
    "        confs = ts[:, 5]\n",
    "        clss = ts[:, 6]\n",
    "        if ts.shape[0] != 0:\n",
    "            for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n",
    "\n",
    "                if cls != 0:\n",
    "                    continue\n",
    "                id_set.add(id)\n",
    "                cv2.rectangle(\n",
    "                    frame,\n",
    "                    (xyxy[0], xyxy[1]),\n",
    "                    (xyxy[2], xyxy[3]),\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f'id: {id}',\n",
    "                    (xyxy[0], xyxy[1]-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "            cv2.putText(\n",
    "                    frame,\n",
    "                    f'Count: {len(id_set)}',\n",
    "                    (10, 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0,0,0),\n",
    "                    thickness\n",
    "                )\n",
    "            cv2.imshow('Frame', frame)\n",
    "    except Exception as e:\n",
    "         print(\"Error:\",e)\n",
    "         pass\n",
    "    ret, frame = cap.read()         \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "            break\n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
