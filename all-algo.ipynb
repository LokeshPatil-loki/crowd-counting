{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Canteen.mp4\"\n",
    "frame_width, frame_height = 1280,720\n",
    "# modelPath = \"./yolo-head-detection.pt\"\n",
    "modelPath = \"./yolov8s.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.0+cpu\n",
      "Is CUDA enabled? False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\",torch.__version__)\n",
    "\n",
    "print(\"Is CUDA enabled?\",torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOTSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-10 16:04:10.609\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\u001b[0m\n",
      "\u001b[32m2024-02-10 16:04:10.611\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m211\u001b[0m - \u001b[33m\u001b[1mThe following layers are discarded due to unmatched keys or layer size: ('classifier.weight', 'classifier.bias')\u001b[0m\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m cap\u001b[38;5;241m.\u001b[39mset(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FRAME_WIDTH,frame_height)\n\u001b[0;32m     28\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread() \n\u001b[1;32m---> 29\u001b[0m tracker \u001b[38;5;241m=\u001b[39m \u001b[43mBoTSORT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mosnet_x0_25_msmt17.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtrack_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mframe_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m thickness \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     31\u001b[0m color \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m255\u001b[39m)\n",
      "File \u001b[1;32m~\\.virtualenvs\\crowdCownting\\Lib\\site-packages\\boxmot\\trackers\\botsort\\bot_sort.py:224\u001b[0m, in \u001b[0;36mBoTSORT.__init__\u001b[1;34m(self, model_weights, device, fp16, track_high_thresh, track_low_thresh, new_track_thresh, track_buffer, match_thresh, proximity_thresh, appearance_thresh, cmc_method, frame_rate, fuse_first_associate, with_reid)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_reid \u001b[38;5;241m=\u001b[39m with_reid\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_reid:\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mReIDDetectMultiBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcmc \u001b[38;5;241m=\u001b[39m SparseOptFlow()\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuse_first_associate \u001b[38;5;241m=\u001b[39m fuse_first_associate\n",
      "File \u001b[1;32m~\\.virtualenvs\\crowdCownting\\Lib\\site-packages\\boxmot\\appearance\\reid_multibackend.py:86\u001b[0m, in \u001b[0;36mReIDDetectMultiBackend.__init__\u001b[1;34m(self, weights, device, fp16)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mand\u001b[39;00m w\u001b[38;5;241m.\u001b[39mis_file() \u001b[38;5;129;01mand\u001b[39;00m w\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     85\u001b[0m         load_pretrained_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, w)\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhalf() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32m~\\.virtualenvs\\crowdCownting\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\crowdCownting\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.virtualenvs\\crowdCownting\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.virtualenvs\\crowdCownting\\Lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\.virtualenvs\\crowdCownting\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\crowdCownting\\Lib\\site-packages\\torch\\cuda\\__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from boxmot import BoTSORT\n",
    "from pathlib import Path\n",
    "frame_width, frame_height = 1280,720\n",
    "model = YOLO(modelPath)\n",
    "\n",
    "def calculate_color_based_on_id(id):\n",
    "  colors = [[255, 0, 0], # Red\n",
    "          [0, 255, 0], # Green|\n",
    "          [0, 0, 255], # Blue\n",
    "          [255, 255, 0], # Yellow\n",
    "          [255, 0, 255], # Magenta\n",
    "          [0, 255, 255], # Cyan\n",
    "          [128, 128, 128], # Gray\n",
    "          [255, 255, 255], # White\n",
    "          [0, 0, 0], # Black\n",
    "          [192, 192, 192]]\n",
    "  return colors[id % len(colors) ]\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(f\"./input_videos/{filename}\")\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_height)\n",
    "ret, frame = cap.read() \n",
    "tracker = BoTSORT(model_weights=Path('osnet_x0_25_msmt17.pt'),device=\"cuda:0\",fp16=False,track_buffer=60*15,frame_rate=60)\n",
    "thickness = 2\n",
    "color = (0,0,255)\n",
    "# cap_out = cv2.VideoWriter(f\"output_videos/botsort_{filename}\", cv2.VideoWriter_fourcc(*'MP4V'), cap.get(cv2.CAP_PROP_FPS),\n",
    "#                           (frame.shape[1], frame.shape[0]))\n",
    "count = 0\n",
    "id_set = set()\n",
    "while(cap.isOpened()):\n",
    "    count +=1 \n",
    "    detections = model.predict(frame,verbose=False)[0]\n",
    "    results = []\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if np.array(detections.boxes.data.tolist()).ndim < 2:\n",
    "        results = [[0, 0, 0, 0, 0.0922948837280273, 0]] \n",
    "    try:\n",
    "        ts = tracker.update(np.array(detections.boxes.data.tolist()), frame)\n",
    "        xyxys = ts[:,0:4].astype('int') # float64 to int\n",
    "        ids = ts[:, 4].astype('int') # float64 to int \n",
    "        confs = ts[:, 5]\n",
    "        clss = ts[:, 6]\n",
    "        if ts.shape[0] != 0:\n",
    "            for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n",
    "\n",
    "                if cls != 0:\n",
    "                    continue\n",
    "                id_set.add(id)\n",
    "                cv2.rectangle(\n",
    "                    frame,\n",
    "                    (xyxy[0], xyxy[1]),\n",
    "                    (xyxy[2], xyxy[3]),\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f'id: {id}',\n",
    "                    (xyxy[0], xyxy[1]-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "            cv2.putText(\n",
    "                    frame,\n",
    "                    f'Count: {len(id_set)}',\n",
    "                    (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0,0,0),\n",
    "                    thickness\n",
    "                )\n",
    "            cv2.putText(\n",
    "                    frame,\n",
    "                    f'Frame: {count}',\n",
    "                    (10, 60),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0,0,0),\n",
    "                    thickness\n",
    "                )\n",
    "            \n",
    "            cv2.imshow('Frame', frame)\n",
    "\n",
    "            # cap_out.write(frame)\n",
    "\n",
    "    except Exception as e:\n",
    "         print(\"Error:\",e)\n",
    "         pass\n",
    "    ret, frame = cap.read()         \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "            break\n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows() \n",
    "print(f\"botsort_{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-04 04:25:41.993\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\u001b[0m\n",
      "\u001b[32m2023-11-04 04:25:41.994\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m211\u001b[0m - \u001b[33m\u001b[1mThe following layers are discarded due to unmatched keys or layer size: ('classifier.weight', 'classifier.bias')\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "15\n",
      "15\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "14\n",
      "14\n",
      "14\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "13\n",
      "13\n",
      "15\n",
      "15\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "10\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "14\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'source' is missing. Using 'source=/home/loki/Development/major-project/RealTime-Webcam/venv/lib/python3.11/site-packages/ultralytics/assets'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "botsort_Canteen.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from boxmot import BoTSORT\n",
    "from pathlib import Path\n",
    "frame_width, frame_height = 1280,720\n",
    "model = YOLO(\"yolo-head-detection.pt\")\n",
    "\n",
    "def calculate_color_based_on_id(id):\n",
    "  colors = [[255, 0, 0], # Red\n",
    "          [0, 255, 0], # Green|\n",
    "          [0, 0, 255], # Blue\n",
    "          [255, 255, 0], # Yellow\n",
    "          [255, 0, 255], # Magenta\n",
    "          [0, 255, 255], # Cyan\n",
    "          [128, 128, 128], # Gray\n",
    "          [255, 255, 255], # White\n",
    "          [0, 0, 0], # Black\n",
    "          [192, 192, 192]]\n",
    "  return colors[id % len(colors) ]\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(f\"./input_videos/{filename}\")\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_height)\n",
    "ret, frame = cap.read() \n",
    "tracker = BoTSORT(model_weights=Path('osnet_x0_25_msmt17.pt'),device=\"cuda:0\",fp16=False)\n",
    "thickness = 2\n",
    "color = (0,0,255)\n",
    "# cap_out = cv2.VideoWriter(f\"output_videos/botsort_{filename}\", cv2.VideoWriter_fourcc(*'MP4V'), cap.get(cv2.CAP_PROP_FPS),\n",
    "#                           (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    id_set = set()\n",
    "    detections = model.predict(frame,verbose=False)[0]\n",
    "    results = []\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if np.array(detections.boxes.data.tolist()).ndim < 2:\n",
    "        results = [[0, 0, 0, 0, 0.0922948837280273, 0]] \n",
    "    try:\n",
    "        ts = tracker.update(np.array(detections.boxes.data.tolist()), frame)\n",
    "        xyxys = ts[:,0:4].astype('int') # float64 to int\n",
    "        ids = ts[:, 4].astype('int') # float64 to int \n",
    "        confs = ts[:, 5]\n",
    "        clss = ts[:, 6]\n",
    "        if ts.shape[0] != 0:\n",
    "            for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n",
    "\n",
    "                if cls != 0:\n",
    "                    continue\n",
    "                id_set.add(id)\n",
    "                cv2.rectangle(\n",
    "                    frame,\n",
    "                    (xyxy[0], xyxy[1]),\n",
    "                    (xyxy[2], xyxy[3]),\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f'id: {id}',\n",
    "                    (xyxy[0], xyxy[1]-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "            print(len(id_set))\n",
    "            cv2.putText(\n",
    "                    frame,\n",
    "                    f'Count: {len(id_set)}',\n",
    "                    (10, 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0,0,0),\n",
    "                    thickness\n",
    "                )\n",
    "            cv2.imshow('Frame', frame)\n",
    "            # cap_out.write(frame)\n",
    "\n",
    "    except Exception as e:\n",
    "         print(\"Error:\",e)\n",
    "         pass\n",
    "    ret, frame = cap.read()         \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "            break\n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows() \n",
    "print(f\"botsort_{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StrongSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-04 04:52:48.231\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\u001b[0m\n",
      "\u001b[32m2023-11-04 04:52:48.232\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m211\u001b[0m - \u001b[33m\u001b[1mThe following layers are discarded due to unmatched keys or layer size: ('classifier.weight', 'classifier.bias')\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: too many indices for array: array is 1-dimensional, but 2 were indexed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'source' is missing. Using 'source=/home/loki/Development/major-project/RealTime-Webcam/venv/lib/python3.11/site-packages/ultralytics/assets'.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from boxmot import BoTSORT,StrongSORT\n",
    "from pathlib import Path\n",
    "model = YOLO(modelPath)\n",
    "\n",
    "def calculate_color_based_on_id(id):\n",
    "  colors = [[255, 0, 0], # Red\n",
    "          [0, 255, 0], # Green\n",
    "          [0, 0, 255], # Blue\n",
    "          [255, 255, 0], # Yellow\n",
    "          [255, 0, 255], # Magenta\n",
    "          [0, 255, 255], # Cyan\n",
    "          [128, 128, 128], # Gray\n",
    "          [255, 255, 255], # White\n",
    "          [0, 0, 0], # Black\n",
    "          [192, 192, 192]]\n",
    "  return colors[id % len(colors) ]\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(f\"./input_videos/{filename}\")\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_height)\n",
    "ret, frame = cap.read() \n",
    "tracker = StrongSORT(model_weights=Path('osnet_x0_25_msmt17.pt'),device=\"cuda:0\",fp16=False)\n",
    "thickness = 2\n",
    "color = (0,0,255)\n",
    "while(cap.isOpened()):\n",
    "    id_set = set()\n",
    "    detections = model.predict(frame,verbose=False)[0]\n",
    "    results = []\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if np.array(detections.boxes.data.tolist()).ndim < 2:\n",
    "        results = [[0, 0, 0, 0, 0.0922948837280273, 0]] \n",
    "    try:\n",
    "        ts = tracker.update(np.array(detections.boxes.data.tolist()), frame)\n",
    "        xyxys = ts[:,0:4].astype('int') # float64 to int\n",
    "        ids = ts[:, 4].astype('int') # float64 to int \n",
    "        confs = ts[:, 5]\n",
    "        clss = ts[:, 6]\n",
    "        if ts.shape[0] != 0:\n",
    "            for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n",
    "\n",
    "                if cls != 0:\n",
    "                    continue\n",
    "                id_set.add(id)\n",
    "                cv2.rectangle(\n",
    "                    frame,\n",
    "                    (xyxy[0], xyxy[1]),\n",
    "                    (xyxy[2], xyxy[3]),\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f'id: {id}',\n",
    "                    (xyxy[0], xyxy[1]-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "            cv2.putText(\n",
    "                    frame,\n",
    "                    f'Count: {len(id_set)}',\n",
    "                    (10, 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0,0,0),\n",
    "                    thickness\n",
    "                )\n",
    "            cv2.imshow('Frame', frame)\n",
    "    except Exception as e:\n",
    "         print(\"Error:\",e)\n",
    "         pass\n",
    "    ret, frame = cap.read()         \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "            break\n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ByteTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "QObject::moveToThread: Current thread (0x55b351395560) is not the object's thread (0x55b350f145c0).\n",
      "Cannot move to target thread (0x55b351395560)\n",
      "\n",
      "qt.qpa.plugin: Could not load the Qt platform plugin \"wayland\" in \"/home/loki/Development/major-project/RealTime-Webcam/venv/lib/python3.11/site-packages/cv2/qt/plugins\" even though it was found.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import argparse\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(f\"./input_videos/{filename}\")\n",
    "# cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_height)\n",
    "\n",
    "\n",
    "# model = YOLO('yolo-head-detection.pt')\n",
    "model = YOLO('yolov8l.pt')\n",
    "\n",
    "CLASS_NAMES_DICT = model.model.names\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "# mask_annotator = sv.MaskAnnotator()\n",
    "tracker = sv.ByteTrack(frame_rate=30,track_buffer=120)\n",
    "print(model.model.names)\n",
    "\n",
    "ret,frame = cap.read()\n",
    "cap_out = cv2.VideoWriter(f\"out.mp4\", cv2.VideoWriter_fourcc(*'MP4V'), cap.get(cv2.CAP_PROP_FPS),\n",
    "                      (frame.shape[1], frame.shape[0]))\n",
    "while ret:\n",
    "    ids = set()\n",
    "    result = model(frame, verbose=False)[0]\n",
    "    detections = sv.Detections.from_ultralytics(result)  \n",
    "    detections = detections[(detections.class_id==0)]\n",
    "    detections = tracker.update_with_detections(detections=detections)\n",
    "    labels  = []\n",
    "    for _, _, confidence, class_id, tracker_id in detections:\n",
    "        labels.append(f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\")\n",
    "        ids.add(tracker_id)\n",
    "    \n",
    "    annotated_image = box_annotator.annotate(frame, detections=detections,labels=labels)\n",
    "    # annotated_image = mask_annotator.annotate(scene=frame,detections=detections,opacity=0.5)\n",
    "    cv2.putText(\n",
    "                    annotated_image,\n",
    "                    f\"Count {len(ids)}\",\n",
    "                    (10, 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0,0,0),\n",
    "                    2\n",
    "                )\n",
    "    cv2.imshow(\"YoloV8\",annotated_image)\n",
    "    cap_out.write(annotated_image)\n",
    "    # ESC -> 27\n",
    "    if cv2.waitKey(30) == 27:\n",
    "        break\n",
    "    ret,frame = cap.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep SORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-04 04:41:13.020\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\u001b[0m\n",
      "\u001b[32m2023-11-04 04:41:13.021\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m211\u001b[0m - \u001b[33m\u001b[1mThe following layers are discarded due to unmatched keys or layer size: ('classifier.weight', 'classifier.bias')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from boxmot import BoTSORT,StrongSORT,DeepOCSORT\n",
    "from pathlib import Path\n",
    "model = YOLO(\"./yolo-head-detection.pt\")\n",
    "\n",
    "def calculate_color_based_on_id(id):\n",
    "  colors = [[255, 0, 0], # Red\n",
    "          [0, 255, 0], # Green\n",
    "          [0, 0, 255], # Blue\n",
    "          [255, 255, 0], # Yellow\n",
    "          [255, 0, 255], # Magenta\n",
    "          [0, 255, 255], # Cyan\n",
    "          [128, 128, 128], # Gray\n",
    "          [255, 255, 255], # White\n",
    "          [0, 0, 0], # Black\n",
    "          [192, 192, 192]]\n",
    "  return colors[id % len(colors) ]\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(f\"./input_videos/{filename}\")\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,frame_height)\n",
    "ret, frame = cap.read() \n",
    "tracker = DeepOCSORT(model_weights=Path('osnet_x0_25_msmt17.pt'),device=\"cuda:0\",fp16=False)\n",
    "thickness = 2\n",
    "color = (0,0,255)\n",
    "while(cap.isOpened()):\n",
    "    id_set = set()\n",
    "    detections = model.predict(frame,verbose=False)[0]\n",
    "    results = []\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if np.array(detections.boxes.data.tolist()).ndim < 2:\n",
    "        results = [[0, 0, 0, 0, 0.0922948837280273, 0]] \n",
    "    try:\n",
    "        ts = tracker.update(np.array(detections.boxes.data.tolist()), frame)\n",
    "        xyxys = ts[:,0:4].astype('int') # float64 to int\n",
    "        ids = ts[:, 4].astype('int') # float64 to int \n",
    "        confs = ts[:, 5]\n",
    "        clss = ts[:, 6]\n",
    "        if ts.shape[0] != 0:\n",
    "            for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):\n",
    "\n",
    "                if cls != 0:\n",
    "                    continue\n",
    "                id_set.add(id)\n",
    "                cv2.rectangle(\n",
    "                    frame,\n",
    "                    (xyxy[0], xyxy[1]),\n",
    "                    (xyxy[2], xyxy[3]),\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f'id: {id}',\n",
    "                    (xyxy[0], xyxy[1]-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    calculate_color_based_on_id(id),\n",
    "                    thickness\n",
    "                )\n",
    "            cv2.putText(\n",
    "                    frame,\n",
    "                    f'Count: {len(id_set)}',\n",
    "                    (10, 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0,0,0),\n",
    "                    thickness\n",
    "                )\n",
    "            cv2.imshow('Frame', frame)\n",
    "    except Exception as e:\n",
    "         print(\"Error:\",e)\n",
    "         pass\n",
    "    ret, frame = cap.read()         \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "            break\n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
